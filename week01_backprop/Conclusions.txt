backprop.ipynb
    1) Для ReLU прямой проход - просто поэлементный максимум между нулем и элементом, обратный проход -
    распределение элементов внешнего градиента (на местах, где были неотрицательные числа, градиент передается без изменений,
    а где были отрицательные числа, передается ноль - реализуется умножением на матрицу с нулями и единицами).

    2) Для слоя Affine прямой проход - вычисление f(x,W)=xW+b, обратный проход - нахождение градиентов через chain rule, т.е.
    представление функции в виде f=dot+b, dot=xW и постепенное нахождение градиентов. Примечание: для db элементы внешнего
    градиента просто складываются.

    3) Для функции потерь используется функция Softmax, у которой формула лдогарифма раскрывается в виде без явного деления.

    4) Особенности реализации:
        1. Создается общий класс слоя Layer и потом классы-наследники типа Dense и ReLU
        2. Каждый класс имеет конструктор, функции forward и backward
        3. Внутри производных классов, если есть обучаемые параметры, заводятся соответствующие поля, которые меняются
        при каждом вызове backwards
        4. Для создания сети достаточно занести слои в список, в цикле вызвать для каждого forward, запомнив входные параметры
        на каждой итерации, и затем снова в цикле вызвать в обратном порядке для каждого слоя backward, используя
        сохраненные входные параметры из прошлого цикла. Тем самым, обновятся все обучаемые параметры в объектах классов.

    5) Для обучения данные разбиваются на мини-батчи, и за i-ую эпоху модель обучается на i-ом мини-батче, где 1 <= i <= число мини-батчей.