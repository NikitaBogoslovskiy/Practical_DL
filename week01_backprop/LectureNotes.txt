1) Структура сетей (юниты и слои)
2) Добавление функций активации для нелинейности (сигмоида, гиперболический тангенс, релу, лики релу) и использование логистической регрессии для вычисления ошибки (softmax) - в обучении используются обе, в тесте вычисления ошибки нет
3) У каждого слоя есть прямое распространение и обратное. При обратном распространении используются правило цепи (локальный градиент в виде транспонированной матрицы Якоби умножается на внешний градиент - в памяти всю матрицу не храним).
4) Производные релу и лики релу, выигрыш последней. Хорошая практика - считать градиент аналитически, проверять через метод конечных разностей (использует определение производной через предел).

Recap:
1) определить слои
2) использовать правило цепи
3) использовать мини-батчи для обучения
4) для каждого мини-батча найти градиент и обновить веса (SGD, Adam, RMSProp, Adagrad и т.д.)

5) Операция copy при прямом распространении для решении нескольких задач (пол и возраст, например). При обратном распространении происходит обычное сложение внешних градиентов.
6) Операция сложения при прямом распространении соответствует операции копирования при обратном.
7) DL пакеты: Tourch, PyTourch, Chainer, Caffe, MxNet, MatConvNet, Tensorflow (Keras, tf.slim), Theano (Lasagne, Keras)
8) Репараметризация может изменить веса, но сохранить выполнение той же функции (*100, /100) - это может привести к медленному градиентному спуску и низкой скорости обучения.
Решения этой проблемы:
1. Инициализация Хавьера (чем больше размерность входного и выходного вектора, тем меньше границы нормального распределения)
2. Использование батч-нормализации (находим мат.ожидание, вычитаем его из изображений, находим стандартное отклонение, делим на корень из него, добавляем два изменяемых параметра - гамма и бета). Нормализация подавляет подобные скачки с резким изменением весов.
3. Инициализация с ортонормированными матрицами (используя SVD) и ренормализация как в батч-нормализации
4. Дропаут. Отключение каких-то юнитов после слоя с активацией (в рамках выставленной вероятности) во время обучения и использование всех юнитов после слоя активации во время теста, но умноженные на используемую вероятность. Имитирует работу нескольких сетей и получает среднее значение их результатов.