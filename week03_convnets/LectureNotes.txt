week03_convnets
    1) Сверточная сеть в середине состоит из модулей, каждый из которых содержит:
        а) сверточный слой:
            - его размер TxCxHxW, где T - число фильтров, C - число каналов в фильтре, H и W - размеры
              фильтра
            - преимущество по сравнению с полносвязным слоем - меньшее число обучаемых параметров
            - помимо размера есть такие параметры, как stride (шаг фильтра) и padding (число нулей, добавляемых по краям)
            - есть два вида сверток: уменьшающие размер (valid mode) и сохраняющие размер (same mode, с помощью паддинга)
            - смысловая интерпретация: каждый фильтр создает свою карту активации для данного изображения, в которой
            отображает локацию некоторого признака, характерного для данного фильтра; при наложении друг на друга карты
            активации образуют изображение, характеризующее всевозможные признаки исходного изображения; при повторении
            свертки и уменьшении изображения/увеличении размера фильтра примитивные признаки перерастают в более сложные
        б) нелинейный слой: ReLU, Leaky ReLU, Maxout (задаются наборы коэффициентов Аi и Bi, которые исходное изображение
        X преобразуют в набор карт, где каждая карта - это Ai*X+Bi; затем создается одна карта, у которой каждая позиция
        k, m - это максимальное значение Ai * Xkm + Bi для всех i, то есть для каждого положения выбирается максимум из
        одной из карт прошлого этапа)
        в) слой пулинга: maxpool (свертка с выбором максимума), sum-pool
    2) Backpropagation для каждой компоненты:
        а) сверточный слой:
            - градиент для входа совпадает с результатом свертки, в которой фильтрами выступают инвертированные фильтры
            с прямого прохода (по строкам и столбцам), а входом свертки - внешний градиент
            - градиент для весов совпадает с результатом свертки, в которой фильтры - это внешний градиент, а вход свертки -
            это входное изображение с прямого прохода
        б) нелинейный слой: для ReLU и Leaky ReLU уже обсуждалось (градиент проходит неизменным, если число положительное,
        либо проходит ноль/малое число, если число отрицательное)
        в) слой пулинга: градиент проходит только по тем позициям, где был максимум при прямом проходе - в остальных
        позициях нули
    3) Сложные места в сети:
        а) начало, когда изображение еще не успело достаточно уменьшиться, а число карт активаций уже сильно выросло -
        возрастает вычислительная сложность; одним из способов ускорить этот этап - представить изображения и фильтры в
        виде двухмерных матриц, чтобы потом их перемножить
        б) конец, где полносвязные слои - большое число обучаемых параметров; помогает дропаут
    4) Примеры сетей:
        а) LeNet - одна из первых сетей с небольшим числом карт активаций и сверточных слоев
        б) AlexNet - 5 сверточных слоев (3х3,3х3,5х5,5х5,5х5), 60 миллионов обучаемых параметров
        в) VGGNet - 16/19 сверточных слоев (все 3х3). 140 миллионов обучаемых параметров
        г) GoogleNet - сеть нелинейна и представляет направленный вычислительный граф, использует свертку только для
        малоразмерных входов (для этого предварительно их уменьшает)
        д) ResNet - 152 слоя, для предотвращения затухания градиента используются модули, которые вставляются в
        сеть, вычисляют некоторый выход (типа Wx -> relu -> Wx) и плюсуют его к исходному входу данного модуля;
        данные модули не обучаются