week04 (fine-tuning)

	Лекция 1
		1) в сверточных слоях для каждой карты активации можно рассмотреть наиболее
		сильно активированные нейроны и восстановить поле восприятие, которое выполнило
		эту активацию (для каждой карты можно рассмотреть поля воспрития из разных
		изображений); так можно узнать, на какие элементы исходного изображения 
		реагирует каждый из фильтров, формирующих карту активации
		
		2) в полносвязных слоях узнать об элементах, активирующих нейроны в картах
		активации, можно с помощью guided-backpropagation: при обратном проходе в слое
		ReLU обнуляться будут не только позиции, бывшие отрицательными при прямом проходе,
		но и позиции, соответствующие отрицательным значениям градиента при обратном проходе;
		тогда при обратном проходе мы получим градиент, визуально отображающий более ярко
		те части изображения, которые вызвали наибольшу активацию
		
		3) схему из пункта 2 можно использовать и для сверточных слоев: для активированных 
		нейронов можно находить как поля восприятия, так и градиенты после guided-backprop`а
		
		4) имея нейронную сеть, можно построить изображение, вызывающее наибольшую активацию 
		в ней: для этого инициализируем изображение нулями или случайными значениями и при 
		помощи градиентного спуска изменяем его, как веса, чтобы добиться наибольшей активации
		
		5) можно создавать галлюцинации для некоторого класса k: инициализируется некоторая 
		матрица весов r нулями или случайным значениями, затем через сеть прогоняются изображения
		x из датасета, функцией потерь выступает min c|r| + L(x+r,k), т.е. наша цель - построить
		такую матрицу r, которая как можно меньше по модулю и сложение которой с любым изображением
		ассоциирует это изображение с классом k (значение потерь L маленькое); изменение матрицы r 
		происходит с помощью градиентного спуска, а сама матрица называется шумом; ее добавление на 		
		любое изображение заствляет классификатор относить его к классу k
		
		6) мы можем визуализировать шумы и обучать модели на них, как на анти-паттернах, но это почти
		не работает
		
		7) сверточная сеть может обучиться практически на любых данных, даже если в них мало смысла
		
		8) для некоторого изображения можно узнать, какие нейроны наиболее сильно активируется, и затем
		с помощью backprop`а построить изображение с нуля, активирующее нейроны в той же мере
		
		9) зная, какие нейроны активируются в наибольшей степени на данном изображении, мы можем создавать
		некоторые шумы, которые вместе с изображением будут активировать эти нейроны еще сильнее;
		результаты совмещения называют накркотиками для нейросети
		
		10) можно создавать изображения, имеющие ту же текстуру, что и исходные: для делаем прямой проход
		через сверточную сеть, находим матрицу Грамма для карт активации (попарное скалярное перемножожение
		карт активации - например, если было 512 карт, то матрица будет размером 512 на 512); берем нулевое
		изображение, также проводим его через сеть и находим матрицу Грамма для него; затем пытаемся снизить
		разницу между матрицами Грамма, выполняя градиентый спуск для нулевого изображения; в итоге, оно
		получит схожую текстуру; похожим образом можно накладывать на одно изображение стиль второго
		
		11) некоторые прикладные задачи могут требовать промежуточных изображений, на которых сработал
		фильтр; например, поиск схожего изображения - в зависимости от его масштаба похожее изображение
		может быть найдено как на последнем слое (если широкий план), так и на промежуточных (если крупный)
		
		12) fine tuning: процедура, основная идея которой взять обученную модель для большой задачи, зафиксировать 
		веса первых n слоев и затем обучить веса оставшихся слоев под более узкую задачу; зафиксированные веса
		можно либо не менять вообще, либо давать возможность скорректироваться под узкую задачу 

	Лекция 2 - семантическая сегментация и детекция объектов, без конспекта