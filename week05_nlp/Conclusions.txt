week05_nlp
	seminar (поиск схожих слов и предложений)
		1) Загрузка текста в виде списка предложений, разбитие всего текста на токены
		2) Обучение модели Word2Vec (https://habr.com/ru/post/446530/) и постановка каждому слову текста некоторого вектора (embedding`а)
		3) Постановка каждому предложению из исходных данных некоторого вектора (средний эмбеддинг для векторов, соответствующих словам в предложении)
		4) Выполнение прикладных задач. Например, поиск схожих предложений - для данного предложения строится эмбеддинг и ищутся ближайшие к нему эмбеддинги,
		соответствующие другим предложения (расстояние мерится через cosine_similarity)
		
	part1_common (бинарная классификация нецензурных предложений)
		1) Загрузка списка предложений и списка лейблов (0 - нет мата, 1 - есть мат)
		2) Разбитие на токены, сортировка и отбор наиболее часто встречающихся n токенов в тексте, формирование из них массива
		3) Постановка каждому предложению вектора длиной n, где i-ая позиция соответствует числу вхождений i-того токена из массива в данное предложение
		4) Создание простейшей модели, в которой есть вход (матрица, состоящая из векторов для каждого предложения), обучаемые весы и известные лейблы (мат или не мат)
		5) Альтернативная постановка векторов предложениям:
			a) постановка каждому слову из всех предложений эмбеддинга (для этого необходимо обучить модель word2vec)
			b) постановка каждому предложению эмбеддинга как суммы эмбеддингов слов, составляющих предложение
		6) Обучение модели из пункта 5 при помощи логистической регрессии (сигмоида -> логрегрессия -> нахождение потерь)
		
	part2_pytorch (предсказание зарплаты по другим данным)
		1) Загрузка таких данных, как Title и Description для каждой вакансии. Загрузка оставшихся данных
		2) Выделение токенов в список, отсеивание самых редких, добавление двух токенов - UNK и PAD - в начало списка, составление словаря, 
		в котором ключ - токен, значение - индекс токена в списке
		3) Сопоставление каждой строке Title вектора, в котором i-ая позиция равна индексу i-того токена из предложения в списке токенов
		(если такого токена нет (например, не попал в список из-за отсеивания), то ставим индекс UNK; если токены закончились, а нужны еще элементы вектора для поддержания
		некоторой длины, добавляем индексы PAD)
		4) Аналогичное с пунктом 4 сопоставление векторов каждой строке Description
		5) Кодирование оставшихся данных также в виде векторов
		6) Создание модели, содержащей следующие компоненты:
			a) получение зарплатных данных для каждого вектора Title (через embedding, conv и global maxpool)
			b) получение зарплатных данных для каждого вектора Description (через embedding, conv и global maxpool)
			c) получение зарплатных данных для векторов оставшихся данных (через линейные слои dense)
			d) конкатенация зарплатных данных из предыдущих трех пунктов
			e) применение линейностей и получение log(1+salary), т.е. логарифма от зарплат
		7) Обучение модели, где две функции потерь - средний квадрат разности правильных зарплат и предсказанных + среднее значение
		абсолютной ошибки (Mean Absolute Error)
		8) Добавление новшеств - батч-нормализация, нелинейности, дропаут, больше слоев и нейронов, других способы пулинга
		
