week05 (nlp)
	1) Для определения схожести двух слов сравнивается контекст их использования
	2) Способы сопоставления словам векторов в n-мерном пространстве (они определяют контекст, и если контекст 
	схожий, то тогда точки в пространстве лежат близко друг к другу):
		a) произвольное сопоставление векторов единиц и нулей (большие ошибки)
		b) ручное составление графов, где слова связаны по смыслу друг с другом (энергозатратно)
		c) составляется матрица для всех n слов текста размером n x m (где m обычно равно 2*n), для каждого
		слова-строки указывается, сколько раз встречалось слово-столбец в тексте до него и после (поэтому 2*n);
		затем к матрице применяется SVD и число столбцов уменьшается до некоторого k, равного, например, 100 -
		тееперь каждому слову соответствует вектор в пространстве
		d) [skip gram] каждому слову сопоставляется вектор-столбец, который затем умножается справа на матрицу, строки которой
		соответствуют всем словам в тексте; в результате получается вектор-столбец, содержащий большие значения
		для слов, встречающихся вокруг данного, и маленькие для всех остальных слов; для получения вероятностей к 
		вектору применяется Softmax, иерархический softmax (возможно, через дерево Хаффмана) или Negatuve sampling; 
		проверить точность работы можно через парные аналогии (Греция-Афины+Осло должно равняться Норвегии);
		вектор-столбец и матрица - обучаемые параметры
		e) [dl convnet] вариант глубокого обучения с учителем; каждому слову из окна или предложения ставится число (например, 
		номер слова в словаре), которому затем ставится в соответствие вектор (какой именно, можно узнать, 
		например, с помощью word2vec в пункте d); задаются обучаемые веса и дальше идет структура cnn: свертки, паддинги (в 
		качестве нуля можно взять какое-то фиксированное слово), махпулинги (если использовались предложения разной 
		длины, то по каждому предложению берется одно максимальное значение), линейности и нелинейности
	3) NLP-модели лучше обучаются без учителя, в отличие от convnets
	
	Статья по Word2Vec: https://habr.com/ru/post/446530/
